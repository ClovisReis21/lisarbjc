{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297c32bc-b090-48fe-b586-ac3baa269b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dependencia\n",
    "# # Instala o findspark\n",
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7667d13d-19c8-4c34-ba22-1c8cc5665b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, DateType, TimestampType\n",
    "from pyspark.sql.functions import col, sum, from_json, unix_timestamp, window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d26d344-8108-4b16-a876-7dce226466a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a91f581-3d26-48c1-b11d-f5c34f85c827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 21:29:51 WARN Utils: Your hostname, cj resolves to a loopback address: 127.0.1.1; using 192.168.15.34 instead (on interface enp2s0)\n",
      "24/05/25 21:29:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/cj/.ivy2/cache\n",
      "The jars for the packages stored in: /home/cj/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-beb94895-e474-44ab-9066-e24e5033b593;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 632ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-beb94895-e474-44ab-9066-e24e5033b593\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/22ms)\n",
      "24/05/25 21:29:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Cria a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"case\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc11c32b-73a6-4eaa-a8b7-17c2930b2398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos criar uma subscrição no tópico que tem o streaming de dados que desejamos \"puxar\" os dados.\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"vendas-deshboard-bronze\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "# latest\n",
    "# earliest\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f8c2d7c-3e53-494b-ae54-111c82094a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dados = StructType([\n",
    "    StructField(\"id_vendedor\", IntegerType(), False),\n",
    "    StructField(\"id_cliente\", IntegerType(), False),\n",
    "    StructField(\"id_produto\", IntegerType(), False),\n",
    "    StructField(\"id_venda\", IntegerType(), False),\n",
    "    StructField(\"quantidade\", IntegerType(), False),\n",
    "    StructField(\"valor_unitario\", DoubleType(), False),\n",
    "    StructField(\"valor_total\", DoubleType(), False),\n",
    "    StructField(\"desconto\", DoubleType(), False),\n",
    "    StructField(\"data\", DateType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267830e8-07fe-4f07-b803-cbd3c79bedfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_vendedor: integer (nullable = true)\n",
      " |-- id_cliente: integer (nullable = true)\n",
      " |-- id_produto: integer (nullable = true)\n",
      " |-- id_venda: integer (nullable = true)\n",
      " |-- quantidade: integer (nullable = true)\n",
      " |-- valor_unitario: double (nullable = true)\n",
      " |-- valor_total: double (nullable = true)\n",
      " |-- desconto: double (nullable = true)\n",
      " |-- data: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Capturamos cada linha de dado (cada valor) como string\n",
    "df_conversao = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse do formato JSON em dataframe\n",
    "df_conversao = df_conversao.withColumn(\"jsonData\", from_json(col(\"value\"), schema_dados)).select(\"jsonData.*\")\n",
    "\n",
    "df_conversao.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ae1514-3a9a-475a-a3de-634325c5000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_conversao_dados = (df_conversao\n",
    "    .select(\n",
    "        col(\"id_vendedor\").alias(\"vendedor\"),\n",
    "        col(\"id_cliente\").alias(\"cliente\"),\n",
    "        col(\"id_produto\").alias(\"produto\"),\n",
    "        col(\"id_venda\").alias(\"venda\"),\n",
    "        col(\"quantidade\").alias(\"quantidade\"),\n",
    "        col(\"valor_unitario\").alias(\"valor_unitario\"),\n",
    "        col(\"valor_total\").alias(\"total\"),\n",
    "        col(\"desconto\").alias(\"desconto\"),\n",
    "        col(\"data\").alias(\"data\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55e53c72-bd2a-43a2-84f5-c4d5db41e97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- produto: integer (nullable = true)\n",
      " |-- QTD: long (nullable = true)\n",
      " |-- VALOR VENDA: double (nullable = true)\n",
      " |-- DESCONTO: double (nullable = true)\n",
      " |-- VALOR FINAL: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Aqui temos o objeto que irá conter nossa análise, o cálculo da média dos volores totais de venda\n",
    "df_media_vendas = (df_conversao_dados\n",
    "                   .select(col('produto'), col('quantidade'),col('total'),col('desconto'),\n",
    "                          ((col('quantidade') * col('total')) - col('desconto')).alias('valor_final'))\n",
    "                       .groupby(\"produto\").agg(\n",
    "                            F.sum(\"quantidade\").alias(\"QTD\")\n",
    "                            ,F.sum(\"total\").alias(\"VALOR VENDA\")\n",
    "                            ,F.sum(\"desconto\").alias(\"DESCONTO\")\n",
    "                            ,F.sum(\"valor_final\").alias('VALOR FINAL')\n",
    "                   )\n",
    "                   .where(F.col(\"produto\").isNotNull())\n",
    "                   .orderBy(F.col(\"VALOR FINAL\").desc())\n",
    ")\n",
    "df_media_vendas.printSchema()\n",
    "df_media_vendas_json = df_media_vendas.select(F.to_json(F.struct(*df_media_vendas.columns)).alias(\"value\"))\n",
    "df_media_vendas_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed610a73-5a51-4b85-8e0b-909348421736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 21:40:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/05/25 21:40:40 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = df_media_vendas_json \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"vendas-deshboard-gold\") \\\n",
    "    .option(\"checkpointLocation\", \"./check.txt\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "# \\\n",
    "#     .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0ce62-48ca-4e2e-b8c9-372b87ab19ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe7a41e8-4a17-493a-94d2-0a49ceb4d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfcd18de-ffa9-49e9-8204-cb0e24b063ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf9779-f88d-40b1-a288-30d7fbffc908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45aadce-748c-49f6-92b9-f0be45824b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c288ed7-746b-41e4-8bc5-10e554b9d234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = df_media_vendas.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcf076-372d-4e08-b1f0-8fb57fe3d5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed37d8-5310-43f2-af81-1df6cdc505d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9e4af-b0f2-42de-9640-099e07d98afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0a72d-37ce-4688-83d8-f99a32c00925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
