{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca02cd8-7bf6-4432-b853-10804a1001c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, DateType, TimestampType, ArrayType\n",
    "from pyspark.sql.functions import col, sum, from_json, unix_timestamp, window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84cdd3a5-43fa-482f-8132-d0758b40b046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 17:53:19 WARN Utils: Your hostname, cj resolves to a loopback address: 127.0.1.1; using 192.168.15.34 instead (on interface enp2s0)\n",
      "24/05/31 17:53:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/31 17:53:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/31 17:53:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/31 17:53:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/05/31 17:53:20 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "# Cria a sessÃ£o Spark\n",
    "spark = SparkSession.builder.appName(\"case-improving\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf856a7b-922d-449a-9032-06eceb7c11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, MapType\n",
    "mapCol = MapType(StringType(),StringType(),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa54803b-f85f-46ff-b7ec-57a41de6d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('properties', ArrayType(MapType(StringType(),StringType()),True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbf63bd6-0f85-4521-b7a0-5b0a98a84fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-----------------------------+\n",
      "|name      |properties                   |\n",
      "+----------+-----------------------------+\n",
      "|Robert    |[{eye -> black, hair -> red}]|\n",
      "|Washington|[{eye -> grey, hair -> grey}]|\n",
      "|Jefferson |[{eye -> , hair -> brown}]   |\n",
      "+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "dataDictionary = [\n",
    "        # ('James',[{'hair':'black','eye':'brown'}]),\n",
    "        # ('Michael',[{'hair':'brown','eye':None}]),\n",
    "        ('Robert',[{'hair':'red','eye':'black'}]),\n",
    "        ('Washington',[{'hair':'grey','eye':'grey'}]),\n",
    "        ('Jefferson',[{'hair':'brown','eye':''}])\n",
    "        ]\n",
    "df = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8e8d4bf-2daa-47e5-acc1-99564328326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'eye': 'black', 'hair': 'red'}]\n",
      "{'eye': 'black', 'hair': 'red'}\n",
      "[{'eye': 'grey', 'hair': 'grey'}]\n",
      "{'eye': 'grey', 'hair': 'grey'}\n",
      "[{'eye': '', 'hair': 'brown'}]\n",
      "{'eye': '', 'hair': 'brown'}\n",
      "[{'eye': 'black', 'hair': 'red'}, {'eye': 'grey', 'hair': 'grey'}, {'eye': '', 'hair': 'brown'}]\n",
      "+-----+-----+\n",
      "|  eye| hair|\n",
      "+-----+-----+\n",
      "|black|  red|\n",
      "| grey| grey|\n",
      "|     |brown|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "output_json = []\n",
    "\n",
    "for row in df.rdd.toLocalIterator():\n",
    "    print(row.properties)\n",
    "    for item in row.properties:\n",
    "        print(item)\n",
    "        output_json.append((item))\n",
    "\n",
    "print(output_json)\n",
    "\n",
    "\n",
    "output_json_df = spark.createDataFrame(data=output_json)\n",
    "output_json_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315599f5-32dd-4ab6-b92c-04e66a0d69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|      name| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| NULL|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|black|\n",
      "|    Robert|hair|  red|\n",
      "|Washington| eye| grey|\n",
      "|Washington|hair| grey|\n",
      "| Jefferson| eye|     |\n",
      "| Jefferson|hair|brown|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name,explode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591dc5d6-dcbf-4968-9ec5-e1d626d835df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      name|map_keys(properties)|\n",
      "+----------+--------------------+\n",
      "|     James|         [eye, hair]|\n",
      "|   Michael|         [eye, hair]|\n",
      "|    Robert|         [eye, hair]|\n",
      "|Washington|         [eye, hair]|\n",
      "| Jefferson|         [eye, hair]|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "df.select(df.name,map_keys(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3676bd0-a32f-47ff-af07-c45de511d8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eye', 'hair']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,map_keys\n",
    "keysDF = df.select(explode(map_keys(df.properties))).distinct()\n",
    "keysList = keysDF.rdd.map(lambda x:x[0]).collect()\n",
    "print(keysList)\n",
    "#['eye', 'hair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec2dc12-2b59-45a7-8fb7-e56f84af9c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      name|map_values(properties)|\n",
      "+----------+----------------------+\n",
      "|     James|        [brown, black]|\n",
      "|   Michael|         [NULL, brown]|\n",
      "|    Robert|          [black, red]|\n",
      "|Washington|          [grey, grey]|\n",
      "| Jefferson|             [, brown]|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "df.select(df.name,map_values(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686ca42-4ffb-43c0-b283-40d54e5906ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc7569-aebf-4efe-bba1-0d11046ef37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba96a1-c775-409b-9820-08d53c50e11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
