{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa387c84-6bcb-4bbd-829f-1eac9ad240e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kafka-python in /home/cj/.local/lib/python3.10/site-packages (2.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in /home/cj/.local/lib/python3.10/site-packages (2.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kafka in /home/cj/.local/lib/python3.10/site-packages (1.3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python && pip install findspark && pip install kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f25587-1bba-41c1-8f54-6839c5f1a560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 21:59:31 WARN Utils: Your hostname, cj resolves to a loopback address: 127.0.1.1; using 192.168.15.34 instead (on interface enp2s0)\n",
      "24/05/15 21:59:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/cj/.ivy2/cache\n",
      "The jars for the packages stored in: /home/cj/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-615d836e-86a8-4305-97be-2199d0375114;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 594ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-615d836e-86a8-4305-97be-2199d0375114\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/24ms)\n",
      "24/05/15 21:59:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.34:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming from Kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x798525789810>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming from Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230cb9e8-a14b-4572-9007-7453ee79f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the streaming_df to read from kafka\n",
    "streaming_df = spark.readStream\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"vandas-deshboard-bronze\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc8e7e7-f6ae-4130-8655-40070c96e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Schema\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "json_schema = StructType([StructField('customerId', StringType(), True), \\\n",
    "StructField('data', StructType([StructField('devices', ArrayType(StructType([ \\\n",
    "StructField('deviceId', StringType(), True), \\\n",
    "StructField('measure', StringType(), True), \\\n",
    "StructField('status', StringType(), True), \\\n",
    "StructField('temperature', LongType(), True)]), True), True)]), True), \\\n",
    "StructField('eventId', StringType(), True), \\\n",
    "StructField('eventOffset', LongType(), True), \\\n",
    "StructField('eventPublisher', StringType(), True), \\\n",
    "StructField('eventTime', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138d6c42-f8b2-43d2-8e65-1364c386dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse value from binay to string\n",
    "json_df = streaming_df.selectExpr(\"cast(value as string) as value\")\n",
    "\n",
    "# Apply Schema to JSON value column and expand the value\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], json_schema)).select(\"value.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "546dfa7f-ef1e-4cfd-9f1e-204f26a1efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explode the data as devices contains list/array of device reading\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "exploded_df = json_expanded_df \\\n",
    "    .select(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"eventTime\", \"data\") \\\n",
    "    .withColumn(\"devices\", explode(\"data.devices\")) \\\n",
    "    .drop(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f0cff5-1dcc-4059-9483-48c1c8d6aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the exploded df\n",
    "flattened_df = exploded_df \\\n",
    "    .selectExpr(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"cast(eventTime as timestamp) as eventTime\", \n",
    "                \"devices.deviceId as deviceId\", \"devices.measure as measure\", \n",
    "                \"devices.status as status\", \"devices.temperature as temperature\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4bd5d82-4380-4f21-975c-e7a8a9704a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the dataframes to find the average temparature\n",
    "# per Customer per device throughout the day for SUCCESS events\n",
    "from pyspark.sql.functions import to_date, avg\n",
    "\n",
    "agg_df = flattened_df.where(\"STATUS = 'SUCCESS'\") \\\n",
    "    .withColumn(\"eventDate\", to_date(\"eventTime\", \"yyyy-MM-dd\")) \\\n",
    "    .groupBy(\"customerId\",\"deviceId\",\"eventDate\") \\\n",
    "    .agg(avg(\"temperature\").alias(\"avg_temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a1035e-0b5d-4dca-a500-027384e38262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 21:59:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/05/15 21:59:41 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/05/15 21:59:41 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/05/15 21:59:41 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/05/15 21:59:41 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/05/15 21:59:41 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+--------+---------+--------+\n",
      "|customerId|deviceId|eventDate|avg_temp|\n",
      "+----------+--------+---------+--------+\n",
      "+----------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the output to console sink to check the output\n",
    "writing_df = agg_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\",\"./tempProducer\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "    \n",
    "# Start the streaming application to run until the following happens\n",
    "# 1. Exception in the running program\n",
    "# 2. Manual Interruption\n",
    "# writing_df.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73852ea-f99b-46b3-b626-bc6e60719c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
