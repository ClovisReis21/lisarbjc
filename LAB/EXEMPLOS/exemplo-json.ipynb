{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7632ae-6baf-483e-8e62-521f07638d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, DateType, ArrayType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa10159-4d0e-46d9-b040-3f871ac684d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 19:22:11 WARN Utils: Your hostname, cj resolves to a loopback address: 127.0.1.1; using 192.168.15.34 instead (on interface enp2s0)\n",
      "24/05/31 19:22:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/31 19:22:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/31 19:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/31 19:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340f91a8-78ae-48ac-90b6-3dc7553a9582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "root\n",
      " |-- id_vendedor: integer (nullable = false)\n",
      " |-- valor_faturado: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|id_vendedor|valor_faturado|\n",
      "+-----------+--------------+\n",
      "|1          |10.0          |\n",
      "|2          |20.0          |\n",
      "|2          |20.0          |\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FUNCIONA\n",
    "json_fonte = ['{\"id_vendedor\":1,\"valor_faturado\":10.0}','{\"id_vendedor\":2,\"valor_faturado\":20.0}','{\"id_vendedor\":2,\"valor_faturado\":20.0}']\n",
    "\n",
    "import json\n",
    "outpur_json = []\n",
    "for item in json_fonte:\n",
    "    outpur_json.append(json.loads(item))\n",
    "print(type(outpur_json))\n",
    "\n",
    "teste_schema_1 = StructType([\n",
    "    StructField(\"id_vendedor\", IntegerType(), False),\n",
    "    StructField(\"valor_faturado\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# teste_data_1 = [(1,10.00),(2,20.00),(2,20.00)]\n",
    "teste_df_1 = spark.createDataFrame(data=outpur_json,schema=teste_schema_1)\n",
    "teste_df_1.printSchema()\n",
    "teste_df_1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7c36299-b7a1-4ee6-b76c-1397c06b4f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_vendedor: integer (nullable = false)\n",
      " |-- valor_faturado: double (nullable = false)\n",
      "\n",
      "+-----------+--------------+\n",
      "|id_vendedor|valor_faturado|\n",
      "+-----------+--------------+\n",
      "|1          |10.0          |\n",
      "|2          |20.0          |\n",
      "|2          |20.0          |\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teste_schema_1 = StructType([\n",
    "    StructField(\"id_vendedor\", IntegerType(), False),\n",
    "    StructField(\"valor_faturado\", DoubleType(), False)\n",
    "])\n",
    "teste_data_1 = [(1,10.00),(2,20.00),(2,20.00)]\n",
    "teste_df_1 = spark.createDataFrame(data=teste_data_1,schema=teste_schema_1)\n",
    "teste_df_1.printSchema()\n",
    "teste_df_1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07d37a70-96ed-463c-b0e9-b7f8c43f0ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|value                                  |\n",
      "+---------------------------------------+\n",
      "|{\"id_vendedor\":1,\"valor_faturado\":10.0}|\n",
      "|{\"id_vendedor\":2,\"valor_faturado\":20.0}|\n",
      "|{\"id_vendedor\":2,\"valor_faturado\":20.0}|\n",
      "+---------------------------------------+\n",
      "\n",
      "root\n",
      " |-- faturamento_desconto: struct (nullable = true)\n",
      " |    |-- id_vendedor: integer (nullable = false)\n",
      " |    |-- valor_faturado: double (nullable = false)\n",
      "\n",
      "+--------------------+\n",
      "|faturamento_desconto|\n",
      "+--------------------+\n",
      "|NULL                |\n",
      "|NULL                |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_teste_df_1 = teste_df_1.select(F.to_json(F.struct(*teste_df_1.columns)).alias(\"value\"))\n",
    "out_teste_df_1.show(truncate=False)\n",
    "\n",
    "str_teste_df_1 = out_teste_df_1.select(\"value\").rdd.flatMap(lambda x: x).collect()\n",
    "str_teste_df_1\n",
    "\n",
    "isto = ([{\"id_vendedor\":1,\"valor_faturado\":10.0},{\"id_vendedor\":2,\"valor_faturado\":20.0}])\n",
    "teste_schema_2 = StructType([\n",
    "    StructField(\"faturamento_desconto\",teste_schema_1,True)\n",
    "])\n",
    "\n",
    "teste_df_1 = spark.createDataFrame(data=isto,schema=teste_schema_2)\n",
    "teste_df_1.printSchema()\n",
    "teste_df_1.show(truncate=False)\n",
    "\n",
    "# saiu_teste_df_1 = str_teste_df_1.select(F.to_json(F.struct(*str_teste_df_1.columns)).alias(\"saiu\"))\n",
    "# saiu_teste_df_1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0cd759-473c-436f-84ea-2bb54bcfa629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38352018-e9fa-476b-abc0-d98754f23758",
   "metadata": {},
   "source": [
    "EXEMPLOS ABAIXO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdc0c18-f02b-4381-ab7a-6405e5c3eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809be52f-d79f-496d-be33-487c1e5ec16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|id |value                                                                      |\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n",
      "+---+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert JSON string column to Map type\n",
    "from pyspark.sql.types import MapType,StringType\n",
    "from pyspark.sql.functions import from_json\n",
    "df2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1187f56a-6c34-41c8-87e8-39f30bc71e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------+\n",
      "|id |value                                                                       |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json,col\n",
    "df2.withColumn(\"value\",to_json(col(\"value\"))) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50642b2d-d1e5-4d55-ab02-698a5619b5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+-----------+\n",
      "|id |Zipcode|ZipCodeType|City       |\n",
      "+---+-------+-----------+-----------+\n",
      "|1  |704    |STANDARD   |PARC PARQUE|\n",
      "+---+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n",
    "    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a384503a-6a7d-4d52-8298-2fb8ee803e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|id |ZipCodeType|\n",
      "+---+-----------+\n",
      "|1  |STANDARD   |\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c7de5f-8c68-4f46-90cd-6f234912929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRUCT<City: STRING, State: STRING, ZipCodeType: STRING, Zipcode: BIGINT>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import schema_of_json,lit\n",
    "schemaStr=spark.range(1) \\\n",
    "    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n",
    "    .collect()[0][0]\n",
    "print(schemaStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976fac4-52fb-4427-99db-53acf703d7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1868c10a-9d28-477c-b099-7b1050a08211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|id |value                                                                      |\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n",
      "+---+---------------------------------------------------------------------------+\n",
      "\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|id |value                                                                       |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
    "df.show(truncate=False)\n",
    "\n",
    "#Convert JSON string column to Map type\n",
    "from pyspark.sql.types import MapType,StringType\n",
    "from pyspark.sql.functions import from_json\n",
    "df2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import to_json,col\n",
    "df2.withColumn(\"value\",to_json(col(\"value\"))) \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import json_tuple\n",
    "df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n",
    "    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import get_json_object\n",
    "df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import schema_of_json,lit\n",
    "schemaStr=spark.range(1) \\\n",
    "    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n",
    "    .collect()[0][0]\n",
    "print(schemaStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c39401-4691-471e-9426-b03aa2163b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070cd7bf-5723-479a-b4c9-790be7c5f647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a46cf0-1e0b-48fb-a67a-1124aaaa2230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fb0c3-8c0d-4d80-8ce1-73d739df0aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe509f5-9ba0-4314-80d8-c14575b06af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ba1f5-4d3f-4d90-bedf-49a09f6a15ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2973ea-ae57-4635-a6ca-7bae0c84efe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017a33a-1d19-4c56-842f-b2f1f3ddfd69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
