{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297c32bc-b090-48fe-b586-ac3baa269b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in /home/cj/.local/lib/python3.10/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Dependencia\n",
    "# Instala o findspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7667d13d-19c8-4c34-ba22-1c8cc5665b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, DateType, TimestampType\n",
    "from pyspark.sql.functions import col, sum, from_json, unix_timestamp, window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d26d344-8108-4b16-a876-7dce226466a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a91f581-3d26-48c1-b11d-f5c34f85c827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 19:28:35 WARN Utils: Your hostname, cj resolves to a loopback address: 127.0.1.1; using 192.168.15.34 instead (on interface enp2s0)\n",
      "24/06/05 19:28:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/cj/.ivy2/cache\n",
      "The jars for the packages stored in: /home/cj/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6d90c992-bd0f-48af-87df-ced9e631c56d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 453ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6d90c992-bd0f-48af-87df-ced9e631c56d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/9ms)\n",
      "24/06/05 19:28:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Cria a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"case\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc11c32b-73a6-4eaa-a8b7-17c2930b2398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos criar uma subscrição no tópico que tem o streaming de dados que desejamos \"puxar\" os dados.\n",
    "from_kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"vendas-deshboard-bronze\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "# latest\n",
    "# earliest\n",
    "from_kafka_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f8c2d7c-3e53-494b-ae54-111c82094a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_kafka_schema = StructType([\n",
    "    StructField(\"id_vendedor\", IntegerType(), False),\n",
    "    StructField(\"id_cliente\", IntegerType(), False),\n",
    "    StructField(\"id_produto\", IntegerType(), False),\n",
    "    StructField(\"id_venda\", IntegerType(), False),\n",
    "    StructField(\"quantidade\", IntegerType(), False),\n",
    "    StructField(\"valor_unitario\", DoubleType(), False),\n",
    "    StructField(\"valor_total\", DoubleType(), False),\n",
    "    StructField(\"desconto\", DoubleType(), False),\n",
    "    StructField(\"data\", DateType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "267830e8-07fe-4f07-b803-cbd3c79bedfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_vendedor: integer (nullable = true)\n",
      " |-- id_cliente: integer (nullable = true)\n",
      " |-- id_produto: integer (nullable = true)\n",
      " |-- id_venda: integer (nullable = true)\n",
      " |-- quantidade: integer (nullable = true)\n",
      " |-- valor_unitario: double (nullable = true)\n",
      " |-- valor_total: double (nullable = true)\n",
      " |-- desconto: double (nullable = true)\n",
      " |-- data: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Capturamos cada linha de dado (cada valor) como string\n",
    "from_kafka_value_str = from_kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse do formato JSON em dataframe\n",
    "from_kafka_bronze_df = from_kafka_value_str.withColumn(\"jsonData\", from_json(col(\"value\"), from_kafka_schema)).select(\"jsonData.*\")\n",
    "\n",
    "from_kafka_bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ae1514-3a9a-475a-a3de-634325c5000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ideal: double (nullable = true)\n",
      " |-- desconto: double (nullable = true)\n",
      " |-- venda: double (nullable = true)\n",
      " |-- percentual_venda: double (nullable = true)\n",
      " |-- percentual_desconto: double (nullable = true)\n",
      " |-- grafico: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold_df = (\n",
    "    from_kafka_bronze_df\n",
    "        .select(col('valor_total').alias('ideal'), 'desconto', (col('valor_total') - col('desconto')).alias('venda'))\n",
    "        .agg(F.sum('ideal').alias('ideal'),\n",
    "             F.sum('desconto').alias('desconto'),\n",
    "             F.sum('venda').alias('venda'))\n",
    "        .select('*'\n",
    "            ,((col('venda') / col('ideal')) * 100).alias('percentual_venda')\n",
    "            ,((col('desconto') / col('ideal')) * 100).alias('percentual_desconto'))\n",
    "        .where(col('ideal').isNotNull())\n",
    "        .withColumn('grafico',F.lit('pizza'))\n",
    "    )\n",
    "gold_df.printSchema()\n",
    "gold_json = gold_df.select(F.to_json(F.struct(*gold_df.columns)).alias(\"value\"))\n",
    "gold_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed610a73-5a51-4b85-8e0b-909348421736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 19:28:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "to_kafka_gold = (gold_json\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .writeStream\n",
    "    # .format(\"console\")\n",
    "    .format(\"kafka\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"vendas-deshboard-gold\")\n",
    "    .option(\"checkpointLocation\", \"./check.txt\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    "    # .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb0ce62-48ca-4e2e-b8c9-372b87ab19ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 19:28:41 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "24/06/05 19:28:42 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((vendas-deshboard-bronze-0,136,0))\n",
      "24/06/05 19:28:42 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets\n",
      "24/06/05 19:28:43 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((vendas-deshboard-bronze-0,136,0))\n",
      "24/06/05 19:28:43 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets\n",
      "24/06/05 19:28:44 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((vendas-deshboard-bronze-0,136,0))\n",
      "24/06/05 19:28:44 WARN KafkaMicroBatchStream: Partition vendas-deshboard-bronze-0's offset was changed from 136 to 0, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/06/05 19:28:44 WARN KafkaMicroBatchStream: Partition vendas-deshboard-bronze-0's offset was changed from 136 to 0, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/06/05 19:28:45 WARN KafkaMicroBatchStream: Partition vendas-deshboard-bronze-0's offset was changed from 136 to 0, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/06/05 19:28:45 WARN KafkaMicroBatchStream: Partition vendas-deshboard-bronze-0's offset was changed from 136 to 0, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/06/05 19:28:45 WARN KafkaMicroBatchStream: Partition vendas-deshboard-bronze-0's offset was changed from 136 to 0, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/06/05 19:28:45 WARN KafkaMicroBatchStream: Partition vendas-deshboard-bronze-0's offset was changed from 136 to 0, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/06/05 19:28:46 WARN HDFSBackedStateStoreProvider: The state for version 108 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mto_kafka_gold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "to_kafka_gold.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a41e8-4a17-493a-94d2-0a49ceb4d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd18de-ffa9-49e9-8204-cb0e24b063ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf9779-f88d-40b1-a288-30d7fbffc908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45aadce-748c-49f6-92b9-f0be45824b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c288ed7-746b-41e4-8bc5-10e554b9d234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = df_media_vendas.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcf076-372d-4e08-b1f0-8fb57fe3d5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed37d8-5310-43f2-af81-1df6cdc505d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9e4af-b0f2-42de-9640-099e07d98afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0a72d-37ce-4688-83d8-f99a32c00925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
